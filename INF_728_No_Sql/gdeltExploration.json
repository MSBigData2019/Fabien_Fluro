{"paragraphs":[{"text":"%md\n## Exploration des donnees GDELT via Spark\nDans ce notebook nous allons commencer a explorer les donnees GDELT qu'on a stoque sur S3","user":"admin","dateUpdated":"2019-01-15T09:56:12+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Exploration des donnees GDELT via Spark</h2>\n<p>Dans ce notebook nous allons commencer a explorer les donnees GDELT qu&rsquo;on a stoque sur S3</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1547542572467_1270894866","id":"20181212-102323_67420128","dateCreated":"2019-01-15T09:56:12+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6803"},{"text":"sc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAJHG3QBM547XMHZWQ\") // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"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\") // mettre votre secret du fichier credentials.csv\n","user":"admin","dateUpdated":"2019-01-15T10:33:33+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1547542572469_193582911","id":"20171217-230735_1688540039","dateCreated":"2019-01-15T09:56:12+0100","dateStarted":"2019-01-15T10:33:33+0100","dateFinished":"2019-01-15T10:33:34+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6804"},{"text":"\nsc.hadoopConfiguration.set(\"fs.s3a.connection.maximum\",\"1000\")","user":"admin","dateUpdated":"2019-01-15T10:33:39+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1547543080305_-1539926071","id":"20190115-100440_403122096","dateCreated":"2019-01-15T10:04:40+0100","dateStarted":"2019-01-15T10:33:39+0100","dateFinished":"2019-01-15T10:33:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6805"},{"text":"%md Les fichiers sont stoquees compresses, on a besoin d'un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu'on les lit depuis S3:","user":"admin","dateUpdated":"2019-01-15T09:56:12+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Les fichiers sont stoquees compresses, on a besoin d&rsquo;un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu&rsquo;on les lit depuis S3:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1547542572470_1840401292","id":"20181212-102329_808049084","dateCreated":"2019-01-15T09:56:12+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6806"},{"text":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\n// 20181201000000.export.CSV.zip\nval textRDD = sc.binaryFiles(\"s3a://fluro-fabien-telecom-gdelt2018/2018120119[0-9]*.export.CSV.zip\"). // charger quelques fichers via une regex\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile(_ != null).\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\ntextRDD.take(1)\n","user":"admin","dateUpdated":"2019-01-15T10:33:45+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\ntextRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[32] at flatMap at <console>:44\ncom.amazonaws.AmazonClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3480)\n  at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:604)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:960)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:734)\n  at org.apache.hadoop.fs.Globber.listStatus(Globber.java:69)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:217)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:294)\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n  at org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)\n  at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1337)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1331)\n  ... 52 elided\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n  at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226)\n  at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195)\n  at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n  at com.amazonaws.http.conn.$Proxy18.getConnection(Unknown Source)\n  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423)\n  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)\n  ... 79 more\n"}]},"apps":[],"jobName":"paragraph_1547542572471_884791133","id":"20171217-232457_1732696781","dateCreated":"2019-01-15T09:56:12+0100","dateStarted":"2019-01-15T10:33:45+0100","dateFinished":"2019-01-15T10:36:20+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6807"},{"text":"val cachedEvents = textRDD.cache","user":"admin","dateUpdated":"2019-01-15T10:33:54+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"cachedEvents: textRDD.type = MapPartitionsRDD[32] at flatMap at <console>:44\n"}]},"apps":[],"jobName":"paragraph_1547543168133_10642646","id":"20190115-100608_1992221392","dateCreated":"2019-01-15T10:06:08+0100","dateStarted":"2019-01-15T10:33:54+0100","dateFinished":"2019-01-15T10:36:21+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6808"},{"text":"case class Event(\nGLOBALEVENTID: Int,\nSQLDATE: Int,\nMonthYear: Int,\nYear: Int,\nFractionDate: Double,\nActor1Code: String,\nActor1Name: String,\nActor1CountryCode: String,\nActor1KnownGroupCode: String,\nActor1EthnicCode: String,\nActor1Religion1Code: String,\nActor1Religion2Code: String,\nActor1Type1Code: String,\nActor1Type2Code: String,\nActor1Type3Code: String,\nActor2Code: String,\nActor2Name: String,\nActor2CountryCode: String,\nActor2KnownGroupCode: String,\nActor2EthnicCode: String,\nActor2Religion1Code: String,\nActor2Religion2Code: String,\nActor2Type1Code: String,\nActor2Type2Code: String,\nActor2Type3Code: String,\nIsRootEvent: Int,\nEventCode: String,\nEventBaseCode: String,\nEventRootCode: String,\nQuadClass: Int,\nGoldsteinScale: Double,\nNumMentions: Int,\nNumSources: Int,\nNumArticles: Int,\nAvgTone: Double,\nActor1Geo_Type: Int,\nActor1Geo_FullName: String,\nActor1Geo_CountryCode: String,\nActor1Geo_ADM1Code: String,\nActor1Geo_ADM2Code: String,\nActor1Geo_Lat: Double,\nActor1Geo_Long: Double,\nActor1Geo_FeatureID: String,\nActor2Geo_Type: Int,\nActor2Geo_FullName: String,\nActor2Geo_CountryCode: String,\nActor2Geo_ADM1Code: String,\nActor2Geo_ADM2Code: String,\nActor2Geo_Lat: Double,\nActor2Geo_Long: Double,\nActor2Geo_FeatureID: String,\nActionGeo_Type: Int,\nActionGeo_FullName: String,\nActionGeo_CountryCode: String,\nActionGeo_ADM1Code: String,\nActionGeo_ADM2Code: String,\nActionGeo_Lat: Double,\nActionGeo_Long: Double,\nActionGeo_FeatureID: String,\nDATEADDED: BigInt,\nSOURCEURL: String\n    )","user":"admin","dateUpdated":"2019-01-15T10:05:18+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Event\n"}]},"apps":[],"jobName":"paragraph_1547542940205_1536952989","id":"20190115-100220_469873230","dateCreated":"2019-01-15T10:02:20+0100","dateStarted":"2019-01-15T10:05:18+0100","dateFinished":"2019-01-15T10:05:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6809"},{"text":"def toDouble(s : String): Double = if (s.isEmpty) 0 else s.toDouble\ndef toInt(s : String): Int = if (s.isEmpty) 0  else s.toInt\ndef toBigInt(s : String): BigInt = if (s.isEmpty) BigInt(0) else BigInt(s)\n\ncachedEvents.map(_.split(\"\\t\")).filter(_.length==61).map(\n    e=> Event(\n        toInt(e(0)),toInt(e(1)),toInt(e(2)),toInt(e(3)),toDouble(e(4)),e(5),e(6),e(7),e(8),e(9),e(10),e(11),e(12),e(13),e(14),e(15),e(16),e(17),e(18),e(19),e(20),\n        e(21),e(22),e(23),e(24),toInt(e(25)),e(26),e(27),e(28),toInt(e(29)),toDouble(e(30)),toInt(e(31)),toInt(e(32)),toInt(e(33)),toDouble(e(34)),toInt(e(35)),e(36),e(37),e(38),e(39),toDouble(e(40)),\n        toDouble(e(41)),e(42),toInt(e(43)),e(44),e(45),e(46),e(47),toDouble(e(48)),toDouble(e(49)),e(50),toInt(e(51)),e(52),e(53),e(54),e(55),toDouble(e(56)),toDouble(e(57)),e(58),toBigInt(e(59)),e(60))\n\n).toDS.createOrReplaceTempView(\"export\")\n spark.catalog.cacheTable(\"export\")","user":"admin","dateUpdated":"2019-01-15T10:06:14+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"toDouble: (s: String)Double\ntoInt: (s: String)Int\ntoBigInt: (s: String)BigInt\n"}]},"apps":[],"jobName":"paragraph_1547543120962_630256311","id":"20190115-100520_1194394350","dateCreated":"2019-01-15T10:05:20+0100","dateStarted":"2019-01-15T10:06:14+0100","dateFinished":"2019-01-15T10:06:24+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6810"},{"text":"def toDouble(s : String): Double = if (s.isEmpty) 0 else s.toDouble\ndef toInt(s : String): Int = if (s.isEmpty) 0  else s.toInt\ndef toBigInt(s : String): BigInt = if (s.isEmpty) BigInt(0) else BigInt(s)\n\nval eventsDf = cachedEvents.map(_.split(\"\\t\")).filter(_.length==61).map(\n    e=> Event(\n        toInt(e(0)),toInt(e(1)),toInt(e(2)),toInt(e(3)),toDouble(e(4)),e(5),e(6),e(7),e(8),e(9),e(10),e(11),e(12),e(13),e(14),e(15),e(16),e(17),e(18),e(19),e(20),\n        e(21),e(22),e(23),e(24),toInt(e(25)),e(26),e(27),e(28),toInt(e(29)),toDouble(e(30)),toInt(e(31)),toInt(e(32)),toInt(e(33)),toDouble(e(34)),toInt(e(35)),e(36),e(37),e(38),e(39),toDouble(e(40)),\n        toDouble(e(41)),e(42),toInt(e(43)),e(44),e(45),e(46),e(47),toDouble(e(48)),toDouble(e(49)),e(50),toInt(e(51)),e(52),e(53),e(54),e(55),toDouble(e(56)),toDouble(e(57)),e(58),toInt(e(59).slice(0, 10)),e(60))\n\n).toDF","user":"admin","dateUpdated":"2019-01-15T10:13:51+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"toDouble: (s: String)Double\ntoInt: (s: String)Int\ntoBigInt: (s: String)BigInt\neventsDf: org.apache.spark.sql.DataFrame = [GLOBALEVENTID: int, SQLDATE: int ... 59 more fields]\n"}]},"apps":[],"jobName":"paragraph_1547543627357_-1688354460","id":"20190115-101347_1954792672","dateCreated":"2019-01-15T10:13:47+0100","dateStarted":"2019-01-15T10:13:51+0100","dateFinished":"2019-01-15T10:13:54+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6811"},{"text":"%sql\nSELECT *\nFROM export \nLIMIT 10","user":"admin","dateUpdated":"2019-01-15T10:18:19+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226)\n\tat org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195)\n\tat sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy18.getConnection(Unknown Source)\n\tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423)\n\tat org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:373)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n\tat $line214437498141.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:38)\n\tat $line214437498141.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:36)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://137.194.88.152:4040/jobs/job?id=4"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1547543188259_-1681474533","id":"20190115-100628_1590254368","dateCreated":"2019-01-15T10:06:28+0100","dateStarted":"2019-01-15T10:18:19+0100","dateFinished":"2019-01-15T10:20:52+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6812"},{"text":"\ncachedEvents.take(1)\n","user":"admin","dateUpdated":"2019-01-15T10:09:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): com.amazonaws.AmazonClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:373)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n\tat $anonfun$1.apply(<console>:38)\n\tat $anonfun$1.apply(<console>:36)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226)\n\tat org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195)\n\tat sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy18.getConnection(Unknown Source)\n\tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423)\n\tat org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)\n\t... 26 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1358)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1331)\n  ... 52 elided\nCaused by: com.amazonaws.AmazonClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:373)\n  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n  at org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n  at $anonfun$1.apply(<console>:38)\n  at $anonfun$1.apply(<console>:36)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n  ... 3 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n  at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226)\n  at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195)\n  at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n  at com.amazonaws.http.conn.$Proxy18.getConnection(Unknown Source)\n  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423)\n  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)\n  ... 26 more\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://137.194.88.152:4040/jobs/job?id=2"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1547543210862_-1722694766","id":"20190115-100650_626186271","dateCreated":"2019-01-15T10:06:50+0100","dateStarted":"2019-01-15T10:09:50+0100","dateFinished":"2019-01-15T10:12:23+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6813"},{"text":"eventsDf.count()","user":"admin","dateUpdated":"2019-01-15T10:14:20+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): com.amazonaws.AmazonClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:373)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n\tat $anonfun$1.apply(<console>:38)\n\tat $anonfun$1.apply(<console>:36)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226)\n\tat org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195)\n\tat sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy18.getConnection(Unknown Source)\n\tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423)\n\tat org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)\n\t... 45 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n  ... 52 elided\nCaused by: com.amazonaws.AmazonClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:373)\n  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n  at org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n  at $anonfun$1.apply(<console>:38)\n  at $anonfun$1.apply(<console>:36)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n  ... 3 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n  at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226)\n  at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195)\n  at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n  at com.amazonaws.http.conn.$Proxy18.getConnection(Unknown Source)\n  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423)\n  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)\n  ... 45 more\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://137.194.88.152:4040/jobs/job?id=3"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1547543337741_-1764480312","id":"20190115-100857_1841247611","dateCreated":"2019-01-15T10:08:57+0100","dateStarted":"2019-01-15T10:14:20+0100","dateFinished":"2019-01-15T10:16:54+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6814"},{"text":"eventsDf.printSchema","user":"admin","dateUpdated":"2019-01-15T10:18:06+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- GLOBALEVENTID: integer (nullable = false)\n |-- SQLDATE: integer (nullable = false)\n |-- MonthYear: integer (nullable = false)\n |-- Year: integer (nullable = false)\n |-- FractionDate: double (nullable = false)\n |-- Actor1Code: string (nullable = true)\n |-- Actor1Name: string (nullable = true)\n |-- Actor1CountryCode: string (nullable = true)\n |-- Actor1KnownGroupCode: string (nullable = true)\n |-- Actor1EthnicCode: string (nullable = true)\n |-- Actor1Religion1Code: string (nullable = true)\n |-- Actor1Religion2Code: string (nullable = true)\n |-- Actor1Type1Code: string (nullable = true)\n |-- Actor1Type2Code: string (nullable = true)\n |-- Actor1Type3Code: string (nullable = true)\n |-- Actor2Code: string (nullable = true)\n |-- Actor2Name: string (nullable = true)\n |-- Actor2CountryCode: string (nullable = true)\n |-- Actor2KnownGroupCode: string (nullable = true)\n |-- Actor2EthnicCode: string (nullable = true)\n |-- Actor2Religion1Code: string (nullable = true)\n |-- Actor2Religion2Code: string (nullable = true)\n |-- Actor2Type1Code: string (nullable = true)\n |-- Actor2Type2Code: string (nullable = true)\n |-- Actor2Type3Code: string (nullable = true)\n |-- IsRootEvent: integer (nullable = false)\n |-- EventCode: string (nullable = true)\n |-- EventBaseCode: string (nullable = true)\n |-- EventRootCode: string (nullable = true)\n |-- QuadClass: integer (nullable = false)\n |-- GoldsteinScale: double (nullable = false)\n |-- NumMentions: integer (nullable = false)\n |-- NumSources: integer (nullable = false)\n |-- NumArticles: integer (nullable = false)\n |-- AvgTone: double (nullable = false)\n |-- Actor1Geo_Type: integer (nullable = false)\n |-- Actor1Geo_FullName: string (nullable = true)\n |-- Actor1Geo_CountryCode: string (nullable = true)\n |-- Actor1Geo_ADM1Code: string (nullable = true)\n |-- Actor1Geo_ADM2Code: string (nullable = true)\n |-- Actor1Geo_Lat: double (nullable = false)\n |-- Actor1Geo_Long: double (nullable = false)\n |-- Actor1Geo_FeatureID: string (nullable = true)\n |-- Actor2Geo_Type: integer (nullable = false)\n |-- Actor2Geo_FullName: string (nullable = true)\n |-- Actor2Geo_CountryCode: string (nullable = true)\n |-- Actor2Geo_ADM1Code: string (nullable = true)\n |-- Actor2Geo_ADM2Code: string (nullable = true)\n |-- Actor2Geo_Lat: double (nullable = false)\n |-- Actor2Geo_Long: double (nullable = false)\n |-- Actor2Geo_FeatureID: string (nullable = true)\n |-- ActionGeo_Type: integer (nullable = false)\n |-- ActionGeo_FullName: string (nullable = true)\n |-- ActionGeo_CountryCode: string (nullable = true)\n |-- ActionGeo_ADM1Code: string (nullable = true)\n |-- ActionGeo_ADM2Code: string (nullable = true)\n |-- ActionGeo_Lat: double (nullable = false)\n |-- ActionGeo_Long: double (nullable = false)\n |-- ActionGeo_FeatureID: string (nullable = true)\n |-- DATEADDED: decimal(38,0) (nullable = true)\n |-- SOURCEURL: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1547543350025_-474269506","id":"20190115-100910_1694303363","dateCreated":"2019-01-15T10:09:10+0100","dateStarted":"2019-01-15T10:18:06+0100","dateFinished":"2019-01-15T10:18:06+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6815"},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547543356976_221466957","id":"20190115-100916_1021162600","dateCreated":"2019-01-15T10:09:16+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6816"},{"text":"%md A vous de jouer ! Utilisez la documentation GDELT(https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/) et commencez a explorer les donnees via les API Spark.","user":"admin","dateUpdated":"2019-01-15T09:56:12+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A vous de jouer ! Utilisez la documentation GDELT(<a href=\"https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/\">https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/</a>) et commencez a explorer les donnees via les API Spark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1547542572472_290168000","id":"20171218-084519_765381887","dateCreated":"2019-01-15T09:56:12+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6817"}],"name":"gdeltExploration","id":"2E3NJ8HYV","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}